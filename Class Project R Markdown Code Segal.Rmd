---
title: "Class Project Practical Machine Learning Class Coursera- Mario Segal"
author: "Mario Segal"
date: "July 24, 2014"
output: html_document
---

##This is the code for my Parctical Machine Larning Project

***Load Required Libraries***
```{r load libraries,warning=FALSE,results='hold'}
require(caret)
require(rattle)
```

***Load and Clean the Data***
```{r load data, echo=FALSE,warning=FALSE}
columns <- c("character","factor","date","Date","character","factor",rep("numeric",153),"factor")
train_raw <- read.csv(url("http://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv"),header=T,strip.white=T,stringsAsFactors=F)
test_raw <- read.csv(url("http://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv"),header=T,strip.white=T,stringsAsFactors=F)

#the data was imported as character in many cases and that is incorrect, let's fix that
#for now colums 6-159 should be numeric
train <- train_raw
bad <- which(sapply(train,is.character))
train[bad[4:36]] <- lapply(train[bad[4:36]],as.numeric)

test <- test_raw
bad1 <- which(sapply(test,is.character))
#no need to convert the test as it looks fine
```

***Explore the Data with Some Nice Charts***
```{r EDA,warning=FALSE,results='hold'}
library(ggplot2); library(gridExtra)
vars <- setdiff(which(sapply(train, function(x) sum(!is.na(x))>0)),1:8)
pdf("Exploratory Charts v1.pdf",width=11,height = 8)
for (i in 1:length(vars)) {
    ch <- ggplot(train,aes_string(x=names(train)[160],fill=names(train)[160],y=names(train)[vars[i]]))+theme_bw()+
      geom_jitter(color="blue",position = position_jitter(width = .1),size=1)+theme(legend.position="none")+ 
      geom_boxplot(alpha=0.5)
 print(ch)
}
dev.off()
```

***Drop Sparse Variables***
```{r more data cleaning,warning=FALSE}
# most varibles are either complete or mostly missing
#I will chose to take the ones with full data
#It turns out the summary variables were very sparse and I am supressing those
full_vars <- which(sapply(train,function(x) sum(is.na(x)))==0)
full_vars <- full_vars[-(1:7)]

train_full <- subset(train,,full_vars)
nearZeroVar(train_full)  #good they have no NAs and have proper variance
```

***Check for Highly Correlated Variables, using Approach from Professor Leek***
```{r check correlations,warning=FALSE,results='hold'}
groups <- c("_belt","_arm","_dumbbell","_forearm")

#Check for highly correlated matrices as per Prof. Leek in the lecture
M <- abs(cor(train_full[which(grepl(groups[1],names(train_full)))]))
diag(M) <- 0
cat(groups[1])
which(M > 0.8,arr.ind=T)
#I see a lot of correlation for belt

M <- abs(cor(train_full[which(grepl(groups[2],names(train_full)))]))
diag(M) <- 0
cat(groups[2])
which(M > 0.8,arr.ind=T)
#I see some correlation for arm

M <- abs(cor(train_full[which(grepl(groups[3],names(train_full)))]))
diag(M) <- 0
cat(groups[3])
which(M > 0.8,arr.ind=T)
#I see some correlation for dumbbell

M <- abs(cor(train_full[which(grepl(groups[4],names(train_full)))]))
diag(M) <- 0
cat(groups[4])
which(M > 0.8,arr.ind=T)
#I see very few for forearm
```

***Reduce Dimensions using PCA by Sensor***
```{r PCA,warning=FALSE,results='hold'}
#create PC's for each sensor
pc_belt <- preProcess(train_full[which(grepl(groups[1],names(train_full)))],method="pca")
pc_belt1 <- predict(pc_belt,train_full[which(grepl(groups[1],names(train_full)))])
names(pc_belt1) <- tolower(paste(names(pc_belt1),groups[1],sep=""))

pc_arm <- preProcess(train_full[which(grepl(groups[2],names(train_full)))],method="pca")
pc_arm1 <- predict(pc_arm,train_full[which(grepl(groups[2],names(train_full)))])
names(pc_arm1) <- tolower(paste(names(pc_arm1),groups[2],sep=""))

pc_dumbbell <- preProcess(train_full[which(grepl(groups[3],names(train_full)))],method="pca")
pc_dumbbell1 <- predict(pc_dumbbell,train_full[which(grepl(groups[3],names(train_full)))])
names(pc_dumbbell1) <- tolower(paste(names(pc_dumbbell1),groups[3],sep=""))

pc_forearm <- preProcess(train_full[which(grepl(groups[4],names(train_full)))],method="pca")
pc_forearm1 <- predict(pc_forearm,train_full[which(grepl(groups[4],names(train_full)))])
names(pc_forearm1) <- tolower(paste(names(pc_forearm1),groups[4],sep=""))
```

***Visualize First 2 PCs per Group***
```{r Plot PCAs,warning=FALSE,results='hold'}
print(ggplot(pc_arm1,aes(x=pc1_arm,y=pc2_arm,color=train_full$classe))+geom_point()+ggtitle("Arm"))
print(ggplot(pc_dumbbell1,aes(x=pc1_dumbbell,y=pc2_dumbbell,color=train_full$classe))+geom_point()+
        coord_cartesian(ylim=c(0,10))+ggtitle("Dumbbell"))
print(ggplot(pc_belt1,aes(x=pc1_belt,y=pc2_belt,color=train_full$classe))+geom_point()+ggtitle("Belt"))
print(ggplot(pc_forearm1,aes(x=pc1_forearm,y=pc2_forearm,color=train_full$classe))+geom_point()+
        coord_cartesian(ylim=c(-5,5))+ggtitle("Forearm"))
#The charts seem to group but not perfectly
```

***Combine PCs together and also Apply the PCs to test set for Eventual Prediction***
```{r Combine PCA,warning=FALSE,results='hold'}
#combine all the PCs into an analysis set
train_pc <- data.frame(cbind(pc_belt1,pc_arm1,pc_dumbbell1,pc_forearm1),classe=train_full$classe)

#Apply the PCS to test set;
test_pc_arm <- predict(pc_arm,newdata=test[row.names(pc_arm$rotation)])
names(test_pc_arm) <- tolower(paste(names(test_pc_arm),"_arm",sep=""))
test_pc_forearm <- predict(pc_forearm,newdata=test[row.names(pc_forearm$rotation)])
names(test_pc_forearm) <- tolower(paste(names(test_pc_forearm),"_forearm",sep=""))
test_pc_dumbbell <- predict(pc_dumbbell,newdata=test[row.names(pc_dumbbell$rotation)])
names(test_pc_dumbbell) <- tolower(paste(names(test_pc_dumbbell),"_dumbbell",sep=""))
test_pc_belt <- predict(pc_belt,newdata=test[row.names(pc_belt$rotation)])
names(test_pc_belt) <- tolower(paste(names(test_pc_belt),"_belt",sep=""))

test_pc <- data.frame(cbind(test_pc_arm,test_pc_belt,test_pc_dumbbell,test_pc_forearm))
```

***Start Modeling Process - Try a CART Tree***
```{r CART Tree,warning=FALSE,results='hold'}
#Always a good idea to start with a tree
set.seed(78786)
model1 <- train(classe~.,data=train_pc,method="rpart") 
model1$finalModel   
fancyRpartPlot(model1$finalModel) #this did not work at all as it is not predicting C or E
confusionMatrix(predict(model1,train_pc),train_pc$classe)
```

***Continue Modeling Process - Try a Random Forest of Trees***
```{r Random Forest,warning=FALSE,results='hold'}
#Lets try a more complicated approach - Random Forest that is more robust
set.seed(46776)
model2 <- train(classe~.,data=train_pc,method="rf") 
print(model2)
confusionMatrix(model2)
confusionMatrix(predict(model2,train_pc),train_pc$classe)
varImp(model2);
plot(varImp(model2),main="Random Forest Model Variable Importance")


#visualize accuracy on a chart I personally like
results2 <- data.frame(actual=train_pc$classe,predicted=predict(model2,train_pc))
ggplot(results2,aes(x=actual,y=predicted,color=actual))+geom_jitter(size=0.5,alpha=0.5)+theme_bw()+theme(legend.position="none")
```

***The Random Forest Worked Great, Hence Create Predictions***
```{r Predict,warning=FALSE,results='hold'}
#predict and generate the cases for evaluation
test_pred1 <- predict(model2,test_pc)
setwd("/Users/mario/Practical-ML/Prediction")
source("/Users/mario/Practical-ML/pml_write_files.R")  #read the function from Prof. Leek on working directory
pml_write_files(test_pred1)
```

***Do Some Visualizations to See How the Top PCs in Importance are Separating***
```{r Final Visualizations,warning=FALSE,results='hold'}
#visualize how the top 2 PCs in variable importance separate
ggplot(pc_dumbbell1,aes(x=pc5_dumbbell,y=pc3_dumbbell,color=train_full$classe))+geom_point()+
  coord_cartesian(ylim=c(0,10))+coord_cartesian(xlim=c(-4,4),ylim=c(0,5))+theme_bw()+
  theme(legend.position="bottom")+
  guides(colour = guide_legend(title.position = "top",title.hjust=0.5))+
  scale_x_continuous("5th Dumbbell Principal Component")+
  scale_y_continuous("3rd Dumbbell Principal Component")+scale_color_discrete("Exercise Class")
```

